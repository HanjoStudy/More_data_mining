---
title: "Data mining Course"
author: "Hanjo Odendaal"
date: "27 February 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rmsfuns)
load_pkg(c('purrr', 'dplyr', 'arules', "arulesViz", "igraph", "twitteR", "dtw", "wavelets"))
```

# Association Rules

General Format is A -> C    

Can generalize A (the antecedent) to a specific variable or value combination so can apply to various datasets

__Search Heuristic__

Basis of an association analysis algorithm is the generation of frequent itemsets. Is an "apriori algorithm", a generate-and-test type of search algorithm. Only after exploring all of the possibilities of associations

* containing k items does it consider those contining K + 1 items. Foreach
```{r}

```

k, all candidates are tested to determine whether they have enough support.

A frequent itemset is a set of items that occur together frequently enough to be considered as a candidate for generating association rules.

## 3 Measures: Support, Confidence, Lift

__Support__ is a measure of how frequently the  items must appear in the whole dataset before  they can be considered as a candidate asso-  ciation rule.

__Support__ for a collection of items is the proportion of all transactions in which the items appear together

* support(A -> C) = P(A U C)

We use small values of __support__ as are not looking for the obvious ones.  The actual association rules that we retain are those that meet a criterion __confidence__. 

__Confidence__ calculates the proportion of transactions containing A that also contain C.

* confidence(A -> C) = P(C|A) = P(A U C)/P(A)

* confidence(A -> C) = support(A -> C)/support(A)

Typically looking for larger values of confidence

__Lift__ is the increased likelihood of C being in a transaction if A is included in the transaction:

* lift(A -> C) = confidence(A -> C)/support(C)

Another measure: __Leverage__, which captures the fact that a higher frequency of A and C with a lower lift may be interesting:

* leverage(A->C) = support(A->C)-support(A) \* support(C)

## Titanic dataset

```{r}
data("Titanic")
df <- as.data.frame(Titanic)

# make new object to hold data
titanic.raw <- NULL

# restructure it as a character matrix
for(i in 1:4) {
   titanic.raw <- cbind(titanic.raw, rep(as.character(df[,i]), df$Freq))
}

titanic.raw <- as.data.frame(titanic.raw)
names(titanic.raw) <- names(df)[1:4]

rules.all <- apriori(titanic.raw)

rules.all

inspect(rules.all)
```

As a common phenomenon for association rule mining, many rules generated above are uninteresting. Suppose that we are interested in only rules with rhs indicating survival, so we set rhs=c("Survived=No", "Survived=Yes") in appearance to make sure that only "Survived=No" and "Survived=Yes" will appear in the rhs of rules. All other items can appear in the lhs, as set with default="lhs". 

In the above result rules.all, we can also see that the left-hand side (lhs) of the first rule is empty. To exclude such rules, we set minlen to 2 in the code below. Moreover, the details of the progress are suppressed with verbose=F. 

After association rule mining, rules are sorted by lift to make high-lift rules appear first.

```{r}
rules <- apriori(titanic.raw, control = list(verbose=F),
                 parameter = list(minlen=2, supp=0.005, conf=0.8),
                 appearance = list(rhs=c("Survived=No", 
                                         "Survived=Yes"),
                                   default="lhs"))

quality(rules) <- round(quality(rules), digits=3)
rules.sorted <- sort(rules, by="lift")

inspect(rules.sorted)
```

When other settings are unchanged, with a lower minimum support, more rules will be produced, and the associations between itemsets shown in the rules will be more likely to be by chance. In the above code, the minimum support is set to 0.005, so each rule is supported at least by 12 (=ceiling(0.005 * 2201)) cases, which is acceptable for a population of 2201. 

__Support__, __confidence__ and __lift__ are three common measures for selecting interesting association rules. Besides them, there are many other interestingness measures, such as 
* chi-square
* conviction
* gini
* leverage

More than 20 measures can be calculated with function `interestMeasure()` in the arules package.

```{r}
interestMeasure(rules.sorted,  transactions = titanic.raw)

quality(rules) <- 
  interestMeasure(rules.sorted,  transactions = titanic.raw)

data("Income")
rules <- apriori(Income)

## calculate a single measure and add it to the quality slot
quality(rules) <- cbind(quality(rules), 
                        hyperConfidence = interestMeasure(rules, measure = "hyperConfidence", 
                                                          transactions = Income))

inspect(head(rules, by = "hyperConfidence"))
```

Some rules generated in the previous section (see rules.sorted, page 89) provide little or no extra information when some other rules are in the result. 

For example, the above rule 2 provides no extra knowledge in addition to rule 1, since rules 1 tells us that all 2nd-class children survived.  Generally speaking, when a rule (such as rule 2) is a super rule of another rule (such as rule 1) and the former has the same or a lower lift, the former rule (rule 2) is considered to be redundant.  Other redundant rules in the above result are rules 4, 7 and 8, compared respectively with rules 3, 6 and 5. 

Below we prune redundant rules. Note that the rules have already been sorted descendingly by lift.

To sum up redundancy, if we see that the confidence of Class = 1st, Sex = Female is 1, then by adding if its an adult or an infant does not matter as it does not add information

```{r}
### Just make 
prune_arule <- function(rules, verbose = T) {
  
  rules <- sort(rules, by="lift")
  subset.matrix <- is.subset(rules.sorted, rules.sorted)
  subset.matrix[lower.tri(subset.matrix, diag = T)] <- NA
  redundant <- colSums(subset.matrix, na.rm = T) >= 1
  if(verbose)
  cat("Removing rules:\n", 
      which(redundant), "\n",
      paste(names(which(redundant)),"\n"))
  
  rules[!redundant]
}

rules.pruned <- prune_arule(rules)

inspect(rules.pruned)
```

__While it is easy to find high-lift rules from data, it is not an easy job to understand the identified rules__. It is not uncommon that the association rules are misinterpreted to find their business meanings.  

For instance, in the above rule list `rules.pruned`, the first rule `{Class=2nd, Age=Child} => {Survived=Yes}` has a confidence of one and a lift of three and there are no rules on children of the 1st or 3rd classes. Therefore, it might be interpreted by users as children of the 2nd class had a higher survival rate than other children. __This is wrong!__ 

* The rule states only that all children of class 2 survived, but provides no information at all to compare the survival rates of different classes. 

To investigate the above issue, we run the code below to find rules whose rhs is `Survived=Yes` and lhs contains `Class=1st`, `Class=2nd`, `Class=3rd`, `Age=Child` and `Age=Adult` only, and which contains no other items (default=`none`). We use lower thresholds for both support and confidence than before to find all rules for children of different classes.

* Remember to also change `supp` and `conf` as we work with different samples in the data
* This will give us a much clearer view of the proportional survival rates between classes

```{r}
# create a function to run a truncated apriori - work out supp and conf - input list of rhs and lhs variables
# function(trunc_list, supp, conf)

rules <- apriori(titanic.raw, 
                 parameter = list(minlen=3, supp=0.002, conf=0.2),
                 appearance = list(rhs=c("Survived=Yes"),
                                   lhs=c("Class=1st", "Class=2nd", "Class=3rd",
                                         "Age=Child", "Age=Adult"),
                                   default="none"), 
                 control = list(verbose=F))

rules.sorted <- rules %>% sort(by="confidence")

inspect(rules.sorted)
```

# Visualizing arules

Here we show some ways to visualize association rules, including scatter plot, balloon plot, graph and parallel coordinates plot. More examples on visualizing association rules can be found in the vignettes of package arulesViz [Hahsler and Chelluboina, 2012] which is included in your folder of class materials for today.

```{r}
library(arulesViz)

# figure 9.1 in PDF
plot(rules.all) 

# figure 9.2 in PDF
plot(rules.all, method="grouped")

# figure 9.3 in PDF
plot(rules.all, method="graph")

# figure 9.4
plot(rules.all, method="graph", control=list(type="items"))

# figure 9.5
plot(rules.all, method="paracoord", control=list(reorder=TRUE))


plot(rules.all, 
     method = "matrix", 
     measure = c("lift", "confidence"), 
     control = list(reorder=TRUE))

plot(rules, 
     method = "matrix3D", 
     measure = "lift", 
     control = list(reorder=TRUE))
```

## Groceries Example

```{r}
# Groceries contains sales data of groceries
# with 9835 transactions and 169 items.
data("Groceries")
summary(Groceries)

# Whole milk most popular. Average transaction
# has less than 5 items.

# We mine association rules using
# Apriori algorithm implemented in arules

rules <- apriori(Groceries,
                 parameter=list(support=0.001,
                                confidence=0.5))

rules %>% 
  sort(by = "lift") %>% 
  head(3) %>% 
  inspect
```

Here we see that when people buy `Instant food products, soda`, it is ~19 more likely (`lift`) on average that they will buy `hamburger meat`

Generic Plot Syntax is:
```{r}
plot(x, method = NULL, measure = "support",
            shading = "lift", interactive = FALSE,
            data = data_set, control = list(reorder=TRUE))
```

* x = set of rules to be visualized
* method = visualization method
* measure and shading = interest measures for plot
* interactive = if we want to interactively explore or just present the rules
* data = transaction data set used to mine rules control = list with further control arguments.

```{r}
plot(rules)

rules %>% quality %>% head

rules %>% 
  plot(., 
     measure=c("support", "lift"), 
     shading="confidence")
```

In a "two-key plot", support and confidence are used for the x and y-axes and the color of the points is used to indicate "order" which is the number of items contained in the rule.

```{r}
rules %>% 
  plot(., 
       shading="order", 
       control=list(main="Two-key plot"))
```

## Last FM example

We look for association rules such as: A = LHS  (buy chips) → B = RHS (buy beer). 

* The left-hand side is the `antecedent`
* the right-hand side is the `consequent`

P(B|A) is the conditional probability of B given A. It expresses the probability of event B (the RHS: buying beer), knowing that event A (the LHS: buying chips) has occurred. Recall from your study of probability that P(B|A) = P(A and B)/P(A).

___SUPPORT:___

P(A): Probability that product A is being purchased (event A)—The proportion of times event A occurred is also referred to as the support of A. It is the relative frequency of 1s in column A of the incidence matrix.

P(B): Probability that product B is being purchased (event B)—The proportion of times event B occurred is referred to as the support of B. It is the relative frequency of 1s in column B of the incidence matrix

P(A and B): Probability that products A and B are being purchased at the same time—The proportion of times events A and B occurred together is referred to as the support of A and B. It is the relative frequency of having 1s (i.e., of co-incidence) in both columns A and B of the incidence matrix.

___CONFIDENCE:___

The conditional probability of B (RHS) given A (LHS) is referred to as the confidence of B. It expresses our confidence that product B gets bought if A has been purchased. If this is a small number, the relationship between antecedent A and consequent B is not very relevant as in this case B is unlikely to occur. The confidence is calculated as the ratio, supp(A = LHS and B = RHS)/supp(A = LHS), with the supports of these two events being obtained from the incidence matrix.

___LIFT:___

The lift of A on B is defined as the ratio: 

* lift(A → B) = P(B|A) / P(B) = P(A and B) / P(A)P(B). 

It compares P(B|A) with P(B). If this ratio is larger than 1, we say that A (LHS) results in an upward lift on B (RHS). Or, knowing that A (the antecedent) has occurred, increases the chance that B (the consequent) occurs. 

The lift of A (LHS) on B (RHS) is calculated as the ratio supp(LHS and RHS)/[supp(LHS)supp(RHS)]. Note that this is the same as the lift of B on A:

* P(A|B)/P(A) = P(A and B)/P(A)P(B).

### Balance between lift and confidence

You want to find antecedents that result in big lifts. But, big lifts are practically relevant only if the consequent has a reasonable chance of occurring. Hence, it is productive to screen for combinations that result in good lift AND high confidence.

```{r}
lastfm <- read.csv("Materials-Session-3-More-Association-Analysis/data/lastfm.csv") %>% 
  distinct(., .keep_all = T)
```

Lets use this to now create a playlist for each of the people and get unique records. We must remember that apriori can only handle `factors`
```{r}

playlist <- 
  lastfm %>% 
  mutate(user = factor(user)) %>% 
  split_by("user") %>% 
  map(~ distinct(.x))

playlist <- split(x=lastfm[,"artist"], f=lastfm$user)
playlist <- lapply(playlist,unique) 
```

Lets look at the first two users and the music they listen to:
```{r}
playlist[c(1,2)]
```

The cool thing about apriori and `arules` is the fact that it can create a transaction matrix from my list of users and they wine they enjoy
```{r}
playlist <- as(playlist,"transactions")
```

```{r}
itemFrequency(playlist)

itemFrequencyPlot(playlist, support = 0.08, cex.names = 1.5)

```

Lets only get build the priori rule so that only rules with support >0.01 and confidence of > 0.50 exists, so we don't have a super rare band that had a one hit wonder

```{r}
musicrules <- apriori(playlist,
                      parameter=list(support=.01,confidence=.5)) 

inspect(musicrules)
```


Filter then only those who had a lift of > 5

```{r}
inspect(subset(musicrules, subset=lift > 5)) 

inspect(sort(subset(musicrules, subset=lift > 5), 
             by="confidence"))
```

Here we see that if we were to recommend pink floyd to those who listen to led zeppelin and the doors, then we would be correct 60% of the time

## Income Example

As second example we use the Adult data set from the UCI machine learning repository; 

This data set was analyzed by Hahsler et al. (2005) in their illustration of the R package arules. The data, taken from the US Census Bureau database, contains 48,842 individuals with data on income and several possible predictors of income such as age, work class, education, and so on. 

The first part of the R program in their paper (copied below) transforms the original variables into 115 binary indicator variables. Income, for example is coded as "small" (US$50,000 or less) and "large" (US$50,000 or more). We skip these details and focus our attention on the resulting transaction (incidence) matrix Adult with its 48,842 rows and 115 columns. Since the dimensions of the matrix are so large, the matrix is stored in a special, sparse, transaction matrix format. The item (column) with the largest number of 1s is "capital − loss = none" (46,560 out of 48,842). The number of 1s on a subject (also referred to as the length) varies between 9 and 13; this narrow band is not surprising as indicator variables were constructed from categorical variables, and we know that summing over indicators of each categorical variable must give one for every subject. But, it is the way how these 0s and 1s coincide that matters. We get a better understanding of how the strings of zeros and ones look like by considering the incidence matrix (which we create from the transaction matrix). The indicator for small income (0/1) is in the penultimat column; the indicator for large income is in the very last column.

__NOTE__ the awesome ordered variable

```{r}

data(AdultUCI)

# look at first three rows
AdultUCI[1:3,]

# remove columns we do not need
AdultUCI <- AdultUCI %>% 
  select(-c(fnlwgt, `education-num`))

# AdultUCI[["fnlwgt"]] <- NULL
# AdultUCI[["education-num"]] <- NULL


AdultUCI[["age"]] <- 
  ordered(cut(AdultUCI[["age"]], 
              c(15, 25, 45, 65, 100)), 
          labels = c("Young", "Middle-aged", "Senior", "Old"))

AdultUCI[["hours-per-week"]] <- 
  ordered(cut(AdultUCI[["hours-per-week"]], 
              c(0, 25, 40, 60, 168)), 
          labels = c("Part-time", "Full-time", "Over-time", "Workaholic"))

AdultUCI[["capital-gain"]] <- 
  ordered(cut(AdultUCI[["capital-gain"]], 
              c(-Inf, 0, median(AdultUCI[["capital-gain"]][AdultUCI[["capital-gain"]] > 0]), Inf)), 
          labels = c("None", "Low", "High"))

AdultUCI[["capital-loss"]] <- 
  ordered(cut(AdultUCI[["capital-loss"]], 
              c(-Inf, 0, median(AdultUCI[["capital-loss"]][AdultUCI[["capital-loss"]] > 0]), Inf)), 
          labels = c("none", "low", "high"))
```

Lets transform this data into a sparse matrix and then into incidence matrix

```{r}
# make it a sparse format
Adult <- as(AdultUCI, "transactions")
Adult
summary(Adult)

# transforms transaction matrix into incidence matrix
aa <- as(Adult,"matrix") 

# print the first two rows of the incidence matrix
aa[1:2,]
# look at frequencies of items
itemFrequencyPlot(Adult[, itemFrequency(Adult) > 0.2], cex.names = 1)
```

Now that we have the sparse matrix, lets run the apriori algorithm

```{r}
rules <- apriori(Adult, 
                 parameter = list(support = 0.01, 
                                  confidence = 0.6))
# how many of them?
rules
# summary of rules
summary(rules)
```

Finally, we search over a subset of all rules (with support greater than 0.01 and confidence greater than 0.60, as specified previously) that have small income on the RHS and achieve a lift larger than 1.2. 

```{r}
rulesIncomeSmall <- subset(rules, subset = rhs %in% "income=small" & lift > 1.2)
inspect(sort(rulesIncomeSmall, by = "confidence")[1:3])

rulesIncomeLarge <- subset(rules, subset = rhs %in% "income=large" & lift > 1.2)
inspect(sort(rulesIncomeLarge, by = "confidence")[1:3])
```


### How to sample

To calculate a reasonable sample size n, we choose a minimum support of 5% tied into the formula you see below. As an acceptable error rate for support we choose 10% and as the confidence level (1 − c) we choose 90%.

```{r}
supp <- 0.05
epsilon <- 0.1
c <- 0.1
n <- -2 * log(c)/ (supp * epsilon^2)
n
```

# Social network analysis
```{r}
g <- graph(c(1,2, 1,3, 2,3, 3,5), n=5)
# and then plot it
plot(g)
```

We are assigning to the variable g a graph that has nodes 

* V = {1,2,3,4,5} (from n=5) and has edges 
* E = {(1,2), (1,3), (2,3), (3,5)} 

The commands `V(g)` and `E(g)` print the list of nodes and edges of the graph g:

```{r}

# vertex sequence:
V(g)

# edge sequence
E(g)

```

```{r}
# You can add nodes and edges to an already
# existing graph by, for example, doing this:
g <- graph.empty() + vertices(letters[1:10], 
                              color="red")
# it creates an empty graph and then adds vertices.
# take a look:
plot(g)
# we add more:
g <- g + vertices(letters[11:20], 
                  color="blue")
# take a look:
plot(g)
# We add edges sampling from the existing
# edges in g:
g <- g + edges(sample(V(g), 30, replace=TRUE), 
               color="green")
# Take a look:
plot(g)

# resulting Vertex sequence:
V(g)

# resulting Edge sequence
E(g)
```


## Loading graphs & GRAPH GENERATORS

Lets load in a "pajek" graph type - they are made for very large graphic networks

```{r}
karate <- read.graph("http://cneurocvs.rmki.kfki.hu/igraph/karate.net", format = "pajek")
plot(karate)
```

igraph implements also many useful graph generators. There are models such as: 

* Edos-Renyi model (ER)
* Barabasi-Albert model (BA)
* Watts-Strogratz model (WS). 

The following commands generate graphs using these models:

```{r}
er_graph <- erdos.renyi.game(100, 2/100)
plot(er_graph)
ws_graph <- watts.strogatz.game(1, 100, 4, 0.05)
plot(ws_graph)
ba_graph <- barabasi.game(100)
plot(ba_graph)

```

We can add attributes to nodes and edges of the graphs. These are useful for selecting certain types of nodes, and for visualization. What these commands do is to generate a random graph with 10 nodes, assigns random colors to the nodes, colors edges joining red nodes in red, and edges joining black nodes in black. All remaining edges are colored grey.

```{r}
g <- erdos.renyi.game(10, 0.5)
plot(g)
# assigns random colors to nodes
V(g)$color <- sample( c("red", "black"), vcount(g), rep=TRUE)
plot(g)

# assign grey to the edges
E(g)$color <- "grey"
plot(g)

# identify red vertices as variable "red"
red <- V(g)[ color == "red" ]
# no changes:
plot(g)

# identify black vertices as variable "bl"
bl <- V(g)[ color == "black" ]
# no changes:
plot(g)

# make edges for red nodes red:
E(g)[ red %--% red ]$color <- "red"

# make edges for black nodes black:
E(g)[ bl %--% bl ]$color <- "black"

# remaining edges are still grey:
plot(g)
```

What the following set of commands do is this: They assign random weights to a lattice graph and then colors the ones having weight over 0.9 red, and the rest grey.

```{r}
g <- graph.lattice( c(10,10) )
# graph that looks like a lattice:
plot(g)
# assign random weights:
E(g)$weight <- runif(ecount(g))
# color all edges grey:
E(g)$color <- "grey"
# plot g:
plot(g)
# color it red if weight > 0.9:
E(g)[ weight > 0.9 ]$color <- "red"
# show g:
plot(g)

```

A very important part in the analysis of networks is being able to visualize them As an example, consider the following commands which render the three graphs depicted below:

```{r}
er_graph <- erdos.renyi.game(100, 2/100)
plot(er_graph, vertex.label=NA, vertex.size=3)

ws_graph <- watts.strogatz.game(1, 100, 4, 0.05)
plot(ws_graph, layout=layout.circle, 
     vertex.label=NA, vertex.size=3)

ba_graph <- barabasi.game(100)
plot(ba_graph, vertex.label=NA, vertex.size=3)

help(igraph.plotting)
```

```{r}
# As another example, consider adding attributes
# to edges for a nicer visualization:

# create a lattice graph:
g <- graph.lattice( c(10,10) )
# show it:
plot(g)

E(g)$weight <- runif(ecount(g))
plot(g)

E(g)$color <- "grey"
plot(g)

E(g)[ weight > 0.9 ]$color <- "red"
plot(g)
# changes the layout; the intersections
# are now the nodes:
plot(g, vertex.size = 2 , vertex.label=NA, 
     layout = layout.kamada.kawai ,
     edge.width=2 + 3*E(g)$weight)

```

## MEASURING GRAPHS

There are many measures that help us understand and characterize networks. 
Some are diameter (and average path length), clustering coefficient (or transitivity), and degree distribution. 

igraph provides functions that compute these measures for you. The functions are: 

* `diameter()`
* `transitivity()`
* `average.path.length()`
* `degree()`
* `degree.distribution()` 

```{r}
?transitivity
?degree
?degree.distribution

g <- graph.lattice(length=100, dim=1, nei=4)
plot(g)
average.path.length(g)

diameter(g)

g <- rewire.edges( g, prob=0.05 )
plot(g)
average.path.length(g)

diameter(g)

# for transitivity():
ws <- watts.strogatz.game(1, 100, 4, 0.05)
transitivity(ws)
plot(ws)

p_hat <- ecount(ws)/(vcount(ws)*(vcount(ws))/2)
p_hat

er <- erdos.renyi.game(100, p_hat)
plot(er)
transitivity(er)


# For degree() and degree.distribution():
g <- graph.ring(10)
plot(g)
degree(g)

ba <- barabasi.game(10000, m=3)
p_hat <- ecount(ba)/ ((vcount(ba)-1)*vcount(ba)/2)
er <- erdos.renyi.game(10000, p_hat)
degree.distribution(er)

g <- make_ring(10)
degree(g)
plot(g)
g2 <- sample_gnp(1000, 10/1000)
plot(g2)
degree_distribution(g2)

# Erdos-Renyi:
hist(degree(er))

# Barabasi-Albert:
hist(degree(ba))

# Erdos-Renyi:
plot(degree.distribution(er))

# Barabasi-Albert:
plot(degree.distribution(ba))


```

## A SECOND SET OF igraph DEMOS

Data format. The data is in 'edges' format meaning that each row records a relationship (edge) between two people (vertices).

Additional attributes can be included. Here is an example:

 Supervisor	Examiner	Grade	Specialization
         AA		    BD		  6	             X	
         BD		    CA		  8	             Y
         AA		    DE		  7	             Y
...		...		...	...
In this anonymized example, we have data on co-supervision with additional information about grades and specialization. 

It is also possible to have the data in a matrix form (see the igraph documentation for details)

```{r}
# Load the data. The data needs to be loaded as a table first: 
bsk <- read.table("http://www.dimiter.eu/Data_files/edgesdata3.txt",
                  sep='\t',
                  dec=',', 
                  header=T)
#specify the path, separator(tab, comma, ...), decimal point symbol, etc.
```

Transform the table into the required graph format:

* the 'directed' attribute specifies whether the edges are directed
```{r}
bsk.network <- graph.data.frame(bsk, directed=F) 

plot(bsk.network)
# prints the list of vertices (people)
V(bsk.network) 
# prints the list of edges (relationships)
E(bsk.network) 
# print the number of edges per vertex 
# (relationships per people)
degree(bsk.network) 

```

Subset the data. If we want to exclude people who are in the network only tangentially (participate in one or two relationships only) we can exclude them by subsetting the graph on the basis of the 'degree':

```{r}
# identify those vertices part of less than three edges
bad.vs <- V(bsk.network)[degree(bsk.network)<3]
# exclude them from the graph
bsk.network <- delete.vertices(bsk.network, bad.vs) 

# Plot the data:
plot(bsk.network)
```

Some details about the graph can be specified in advance. For example we can separate some vertices (people) by color:  

* useful for highlighting certain people. Works by matching the name attribute of the vertex to the one specified in the 'ifelse' expression. If name is 'CA'they are colored blue, otherwise red

```{r}
V(bsk.network)$color <- ifelse(V(bsk.network)$name=='CA', 'blue', 'red') 
plot(bsk.network)

# We can also color the connecting edges differently
# depending on the 'grade': 
E(bsk.network)$color<-ifelse(E(bsk.network)$grade==9, "red", "grey")
plot(bsk.network)

# or depending on the different specialization ('spec'):
E(bsk.network)$color<-ifelse(E(bsk.network)$spec=='X', "red", ifelse(E(bsk.network)$spec=='Y', "blue", "grey"))
plot(bsk.network)

# Getting the right scale gets some playing around 
# with the parameters of the scale function 
# (from the 'base' package):
V(bsk.network)$size<-degree(bsk.network)/10
plot(bsk.network)

```

And the final plot: This specifies the size of the margins. The default settings leave too much free space on all sides (if no axes are printed)

```{r}

par(mai=c(0,0,1,0)) 

# here is the graph to be plotted:
plot(bsk.network,
     # the layout method
     # see the igraph documentation for details
     layout = layout.fruchterman.reingold,
     # specifies the title
     main = 'Organizational network example',
     # puts the name labels slightly off the dots
     vertex.label.dist = 0.5,	
     # the color of the border of the dots 
     vertex.frame.color = 'blue',
     # the color of the name labels
     vertex.label.color = 'black',		
     # the font of the name labels
     vertex.label.font = 2,
     # specifies the lables of the vertices. 
     # in this case the 'name' attribute is used
     vertex.label = V(bsk.network)$name,
     # specifies size of the font of the labels. 
     # can also be made to vary
     vertex.label.cex = 1			
)

```

## Twitter example

We first build a network of terms based on their co-occurrence in the same tweets, and then build a network of tweets based on the terms shared by them. And lastly, we build a two-mode network composed of both terms and tweets. We also demonstrate some tricks to plot nice network graphs. 

```{r}
# free memory
rm(list = ls())
gc()
```

At first, a term-document matrix, termDocMatrix.rdata, is loaded into R. After that, it is transformed into a term-term adjacency matrix, based on which a graph is built. Then we plot the graph to show the relationshipbetween frequent terms, and also make the graph more readable by setting colors, font sizes and transparency of vertices and edges.

```{r}
load("Materials-Session-4-Social-Network-Analysis-I-Graph-Visualizations/Data Objects/rdmTweets.RData")
load("Materials-Session-4-Social-Network-Analysis-I-Graph-Visualizations/Data Objects/termDocMatrix.rdata")

# inspect part of the matrix
termDocMatrix[5:10,1:20]

# change it to a Boolean matrix
termDocMatrix[termDocMatrix >= 1] <- 1
termDocMatrix[5:10, 1:20]

```

%*% is an operator for the product of two matrices, and t() transposes amatrix. Here we build a term-term adjacency matrix, where the rows and columns represent terms, and every entry is the number of concurrences of two terms.

This will help us see when one word is used with another

```{r}
# transform into a term-term adjacency matrix
termMatrix <- termDocMatrix %*% t(termDocMatrix)

# inspect terms numbered 5 to 10
termMatrix[5:10,5:10]

# build a graph from the above matrix
g <- graph.adjacency(termMatrix, weighted = T, mode = "undirected")
plot(g)

# remove loops
g <- simplify(g)
plot(g)

# set labels and degrees of vertices
V(g)$label <- V(g)$name
V(g)$degree <- degree(g)
V(g)
plot(g)

```

After that, we plot the network with layout.fruchterman.reingold

```{r}
# set seed to make the layout reproducible
set.seed(3952)
layout1 <- layout.fruchterman.reingold(g)
plot(g, layout = layout1)

plot(g, layout = layout.kamada.kawai)
#tkplot(g, layout = layout.kamada.kawai)


# Next, we set the label size of vertices based on 
# their degrees, to make important terms stand out.
V(g)$label.cex <- 2.2 * V(g)$degree / max(V(g)$degree)+ .2
plot(g)

# Function rgb(red, green, blue, alpha) defines a color,
# with an alpha transparency.
V(g)$label.cex <- 2.2 * V(g)$degree / max(V(g)$degree)+ .2
V(g)$label.color <- rgb(0, 0, .2, .8)
V(g)$frame.color <- NA
plot(g)

# We also set the width and transparency of edges 
# based on their weights. This is useful in applications
# where graphs are crowded with many vertices and edges. 

egam <- (log(E(g)$weight)+.4) / max(log(E(g)$weight)+.4)
E(g)$color <- rgb(.5, .5, 0, egam)
E(g)$width <- egam
# plot the graph in layout1
plot(g, layout=layout1)

```

### NETWORK OF TWEETS

We can also build a graph of tweets base on the number of terms that they have in common. Because most tweets contain one or more words from "r", data" and "mining", most tweets are connected with others and the graph of tweets is very crowded. 

To simplify the graph and to find relationship between tweets beyond the above three keywords, were move the three words before building a graph.

```{r}
load("Materials-Session-4-Social-Network-Analysis-I-Graph-Visualizations/Data Objects/rdmTweets.RData")
# remove "r", "data" and "mining"
idx <- which(dimnames(termDocMatrix)$Terms %in% c("r", "data", "mining"))
M <- termDocMatrix[-idx,]
M[5:10,1:20]

# build a tweet-tweet adjacency matrix
tweetMatrix <- t(M) %*% M
tweetMatrix[5:10,1:20]


g <- graph.adjacency(tweetMatrix, weighted=T, 
                     mode = "undirected")
V(g)$degree <- degree(g)
g <- simplify(g)
plot(g)

# set labels of vertices to tweet IDs
V(g)$label <- V(g)$name
V(g)$label.cex <- 1
V(g)$label.color <- rgb(.4, 0, 0, .7)
V(g)$size <- 2
V(g)$frame.color <- NA
plot(g)

barplot(table(V(g)$degree))

```

Next, we have a look at the distribution of degree of vertices. We can see that there are around 40 isolated vertices (with a degree of zero). Note that most of them are caused by the removal of the three keywords, "r", "data" and "mining".

```{r}
idx <- V(g)$degree == 0
V(g)$label.color[idx] <- rgb(0, 0, .3, .7)
plot(g)
```

```{r}
# convert tweets to a data frame
df <- do.call("rbind", lapply(rdmTweets, as.data.frame))

# set labels to the IDs and the first 20 characters of tweets
V(g)$label[idx] <- paste(V(g)$name[idx], substr(df$text[idx], 1, 20), sep=": ")
egam <- (log(E(g)$weight)+.2) / max(log(E(g)$weight)+.2)
E(g)$color <- rgb(.5, .5, 0, egam)
E(g)$width <- egam

# then run the plot
set.seed(3152)
layout2 <- layout.fruchterman.reingold(g)
plot(g, layout=layout2) 

```

The vertices in crescent are isolated from all others, and next we remove them from graph with function delete.vertices() and re-plot the graph.


```{r}
g2 <- delete.vertices(g, V(g)[degree(g)==0])
# run plot again
plot(g2, layout=layout.fruchterman.reingold) 
```

Similarly, we can also remove edges with low degrees to simplify the graph. Below with function delete.edges(), we remove edges which have weight  of one. After removing edges, some vertices become isolated and are also removed. 

```{r}
g3 <- delete.edges(g, E(g)[E(g)$weight <= 1])
g3 <- delete.vertices(g3, V(g3)[degree(g3) == 0])
plot(g3, layout=layout.fruchterman.reingold) 
```

There are some groups (or cliques) of tweets. Let's have a look at the group in the middle left of the figure.

```{r}
df$text[c(7,12,6,9,8,3,4)]

# easier with a for statement
for (i in c(7,12,6,9,8,3,4)) {
  cat(paste("[", i, "] ", sep=""))
  writeLines(strwrap(df$text[i], 76))
  cat("\n")
}
```

We can see that tweets 7, 12, 6, 9, 8, 3, 4 are on parallel Computing with R. We can also see some other groups below:
  * Tweets 4, 33, 94, 29, 18 and 92: tutorials for R;
  * Tweets 4, 5, 154 and 71: R packages;
  * Tweets 126, 128, 108, 136, 127, 116, 114 and 96: time series analysis;
  * Tweets 112, 129, 119, 105, 108 and 136:  R code examples; and Tweets 27, 24, 22,153, 79, 69, 31, 80, 21, 29, 16, 20, 18, 19 and 30: social network
  analysis.
  * Tweet 4 lies between multiple groups, because it contains keywords "parallel computing", "tutorial" and "package".

# Processing twitter data

# Time series

```{r}
#www <- "http://elena.aut.ac.nz/~pcowpert/ts/Maine.dat"
Maine.month <- read.table("Time_series/Data/Maine_dat.txt", header = TRUE)

# so can manipulate single variable in data frame
attach(Maine.month)
class(Maine.month)
# is a data frame
# We convert it to a time series object
Maine.month.ts <- ts(unemploy, 
                     start = c(1996, 1), 
                     freq = 12)
# take a look
Maine.month.ts

# we divide by 12 to get a mean annual rate:
Maine.annual.ts <- aggregate(Maine.month.ts)/12
# take another look
Maine.annual.ts

```

```{r}
layout(1:2)
plot(Maine.month.ts, ylab = "unemployed (%)")
plot(Maine.annual.ts, ylab = "unemployed (%)")
layout(1:1)

```

We calculate the precise percentages with window(). This function will extract that part of the time series between specified start and end points and will sample with an  interval equal to frequency if its argument is set to TRUE. So, the first lines below gives a time series of February figures.
```{r}
Maine.Feb <- window(Maine.month.ts, 
                    start = c(1996,2), 
                    freq = TRUE)
Maine.Feb
Maine.Aug <- window(Maine.month.ts, 
                    start = c(1996,8), 
                    freq = TRUE)
Maine.Aug
Feb.ratio <- mean(Maine.Feb) / mean(Maine.month.ts)
Aug.ratio <- mean(Maine.Aug) / mean(Maine.month.ts)
Feb.ratio
Aug.ratio
detach(Maine.month)
```


On average, unemployment is 22% higher in  February and 18% lower in August. A possible explanation is that Maine attracts tourists  during the summer, and this creates more jobs.  Also, the period before Christmas and over the New Year's holiday tends to have higher employment rates than the first few months of the new year. 

If we had sampled the data in August of each year, for example, rather than taking yearly averages, we would have  consistently underestimated the unemployment rate by a factor of  about 0.8. The monthly unemployment rate for  all of the United States from January 1996 until October 2006 is plotted below.  The decrease in the unemployment rate around the millennium is common to Maine and the United States as a whole, but  Maine does not seem to be sharing the current US decrease in unemployment.

```{r}
US.month <- read.table("Time_series/Data/USunemp_dat", header = T)
# also in your folder as "USunemp_dat")
attach(US.month)
US.month.ts <- ts(USun, start=c(1996,1), 
                  end=c(2006,10), freq = 12)
US.month.ts
layout(1:1)
plot(US.month.ts, ylab = "unemployed (%)")
detach(US.month)

```

## Airline example

```{r}
a <- ts(1:30, frequency=12, start=c(2011,3))
print(a)
str(a)
attributes(a)


# It has 144(=12*12) values.
plot(AirPassengers)

# decompose() breaks it into various components
apts <- ts(AirPassengers, frequency=12);apts
f <- decompose(apts);f
# seasonal figures
f$figure
plot(f$figure, type="b", xaxt="n", xlab="")
# get names of 12 months in English words
monthNames <- months(ISOdate(2011,1:12,1))
# label x-axis with month names 
# las is set to 2 for vertical label orientation
axis(1, at=1:12, labels=monthNames, las=2) 

# Time series decomposition figure:
plot(f)

```

### Forecasting

```{r}
fit <- arima(AirPassengers, order=c(1,0,0), 
             list(order=c(2,1,0), period=12))
fit
fore <- predict(fit, n.ahead=24)
fore
# error bounds at 95% confidence level
U <- fore$pred + 2*fore$se
L <- fore$pred - 2*fore$se

ts.plot(AirPassengers, fore$pred, U, L, col=c(1,2,4,4), 
        lty = c(1,1,2,2))
legend("topleft", c("Actual", "Forecast", "Error Bounds (95% Confidence)"),
       col=c(1,2,4), lty=c(1,1,2))
```

# Example Multiple time series: Electricity, beer and chocolate data

```{r}
CBE <- read.table("Time_series/Data/cbe_dat", header = T)
# also in your folder as "cbe_dat"
CBE[1:4, ]
class(CBE)
# Now create time series objects for the 
# electricity, beer, and chocolate data.

Elec.ts <- ts(CBE[, 3], 
              start = 1958, 
              freq = 12)
Elec.ts
Beer.ts <- ts(CBE[, 2], 
              start = 1958, 
              freq = 12)
Beer.ts
Choc.ts <- ts(CBE[, 1], 
              start = 1958, 
              freq = 12)
Choc.ts
plot(cbind(Elec.ts, Beer.ts, Choc.ts))

```
 
The three series constitute a multiple  time series. There are many functions in R for handling more than one series,  including ts.intersect to obtain the intersection of two series that overlap  in time. 

```{r}
data(AirPassengers)
AP <- AirPassengers
AP

# The intersection between the air passenger
# data and the electricity data is obtained as follows:
AP.elec <- ts.intersect(AP, Elec.ts)
AP.elec
# Now check your output:
start(AP.elec)
# [1] 1958 1
end(AP.elec)
# [1] 1960 12
AP.elec[1:3, ]
#       AP Elec.ts
# [1,] 340 1497
# [2,] 318 1463
# [3,] 362 1648

# the data for each series are extracted 
# and plotted
AP <- AP.elec[,1]; Elec <- AP.elec[,2]
layout(1:2)
plot(AP, main = "", ylab = "Air passengers / 1000's")
plot(Elec, main = "", ylab = "Electricity production / MkWh")
layout(1:1)

plot(as.vector(AP), as.vector(Elec),
     xlab = "Air passengers / 1000's",
     ylab = "Electricity production / MWh")
abline(reg = lm(Elec ~ AP))
layout(1:1)
```


### Time Series Clustering

Partitions time series data into groups  based on similarity or distance, so that time series in the same cluster are  similar to each other.  Measures of distance (dissimilarity) include Euclidean distance, Manhattan distance, Maximum norm, Hamming distance, the angle between two vectors  (inner product), and Dynamic Time Warping (DTW) distance. 

__Dynamic Time Warping Distance Package __

`dtw` finds optimal alignment  between two time series.  function `dtw(x, y, ...)` computes dynamic time warp and finds optimal alignment between two time series x and y.  function `dtwDist(mx, my=mx, ...)` or  `dist(mx, my=mx, method="DTW", ...)` calculates the distances between time series `mx` and `my`.

```{r}

idx <- seq(0, 2*pi, len=100);idx
a <- sin(idx) + runif(100)/10;a
b <- cos(idx);b
align <- dtw(a, b, step = asymmetricP1, keep=T);align
# Alignment with dynamic time warping:
dtwPlotTwoWay(align)
```

#### Synthetic Control Chart Time Series Data

This dataset contains 600 examples of control charts synthetically generated by the process in Alcock and Manolopoulos (1999). 

http://kdd.ics.uci.edu/databases/synthetic_control/synthetic_control.html

Each control chart is a time series of 60 values, and there are six classes:

* 001-100: Normal
* 101-200: Cyclic
* 201-300: Increasing Trend
* 301-400: Decreasing Trend
* 401-500: Upward shift; and
* 501-600: Downward shift
```{r}
# read in synthetic_control_data.txt
sc <- read.table("Time_series/Data/synthetic_control_data.txt", 
                 header=F, sep="");sc
# show one sample from each class
idx <- c(1,101,201,301,401,501);idx
sample1 <- t(sc[idx,]);sample1
# Plot six classes in synthetic control chart time series
plot.ts(sample1, main="")
```

* Lets see if we can cluster these now with euclidean distance

```{r}
# We set a seed for replicability
set.seed(6218)
# We select 10 cases randomly from each class
n <- 10;n
s <- sample(1:100, n);s
idx <- c(s, 100+s, 200+s, 300+s, 400+s, 500+s);idx
sample2 <- sc[idx,];sample2
observedLabels <- rep(1:6, each=n)

# hierarchical clustering with Euclidean distance
hc <- hclust(dist(sample2), method = "average")
plot(hc, labels = observedLabels, main="")

# cut tree to get 6 clusters
rect.hclust(hc, k=6)
memb <- cutree(hc, k=6)
table(observedLabels, memb)
```

* plot shows shows increasing trend (class 3) and upward shift (class 5) are not well separated, and decreasing trend (class 4) and downward shift (class 6) are also mixed

__DTW clustering__

```{r}
distMatrix <- dist(sample2, method = "DTW")
hc <- hclust(distMatrix, method = "average")
plot(hc, labels = observedLabels, main="")

# cut tree to get 6 clusters
rect.hclust(hc, k=6)
memb <- cutree(hc, k=6)
table(observedLabels, memb)

```

We note that DTW distance is better than Euclidean distance for measuring the similarity betweem time series

##### Time Series Classification

Time series classification models are based on labeled time series and we use these model to predict the label of unlabeled time series.  New features extracted from time series may help to improve the performance of classification models.  Techniques for feature extraction include:

* Singular Value Decomposition (SVD)
* Discrete Fourier Transform  (DFT) 
* DiscreteWavelet Transform (DWT) 
* Piecewise  Aggregate Approximation (PAA)
* Perpetually Important Points (PIP)
* Piecewise Linear Representation 
* Symbolic Representation.

We use `ctree()` from package party [Hothorn et al., 2010] to demonstrate classification of time series with the the original data. The class labels are changed  into categorical values before feeding the data into `ctree()`, so that we won't get class labels as a real numbers like 1.35

```{r}
classId <- rep(as.character(1:6), each=100)
classId
newSc <- data.frame(cbind(classId, sc))
head(newSc)

library(party)
ct <- ctree(classId ~ ., data=newSc, 
            controls = ctree_control(minsplit=30, 
                                     minbucket=10, 
                                     maxdepth=5))
pClassId <- predict(ct)
pClassId
table(classId, pClassId)
# accuracy
(sum(classId==pClassId)) / nrow(sc)

plot(ct, ip_args=list(pval=FALSE), 
     ep_args=list(digits=0))
```

Next, we use DWT (Discrete Wavelet Transform) [Burrus et al., 1998] to extract features from time series and then build a classification model.  

Wavelet transform provides a multi-resolution representation using wavelets. An example of  Haar Wavelet Transform, the simplest DWT, is available at [](http://dmr.ath.cx/gfx/haar/).  Another popular feature extraction technique is Discrete Fourier Transform (DFT) [Agrawal et al., 1993]. An example on extracting DWT coefficients is shown below. Package wavelets [Aldrich, 2010] is used for discrete wavelet transform. In the package, function `dwt(X, filter, n.levels, ...)` computes the discrete wavelet transform coefficients, where `X` is a univariate or multi-variate time series, `filter` indicates which wavelet filter to use, and `n.levels` specifies the level of decomposition. 

It returns an object of class `dwt`, whose slot `W` contains wavelet coefficients and `V`contains scaling coefficients. The original time series can be reconstructed via an inverse discrete wavelet transform with function `idwt()`.

```{r}
# set up structure to hold results
wtData <- NULL
for (i in 1:nrow(sc)) {
  a <- t(sc[i,])
  wt <- dwt(a, filter = "haar", boundary = "periodic")
  wtData <- rbind(wtData, unlist(c(wt@W, wt@V[[wt@level]])))
}
wtData <- as.data.frame(wtData)
head(wtData)
wtSc <- data.frame(cbind(classId, wtData))
head(wtSc)

# build a decision tree with DWT coefficients
ct <- ctree(classId ~ ., data=wtSc, 
            controls = ctree_control(minsplit=30, 
                                     minbucket=10, 
                                     maxdepth=5))
pClassId <- predict(ct)
table(classId, pClassId)
(sum(classId==pClassId)) / nrow(wtSc)
plot(ct, ip_args=list(pval=FALSE), ep_args=list(digits=0))
```




