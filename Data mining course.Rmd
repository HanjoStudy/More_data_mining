---
title: "Data mining Course"
author: "Hanjo Odendaal"
date: "27 February 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rmsfuns)
load_pkg(c('purrr', 'dplyr', 'arules', "arulesViz", "igraph"))
```

# Association Rules

General Format is A -> C    

Can generalize A (the antecedent) to a specific variable or value combination so can apply to various datasets

__Search Heuristic__

Basis of an association analysis algorithm is the generation of frequent itemsets. Is an "apriori algorithm", a generate-and-test type of search algorithm. Only after exploring all of the possibilities of associations containing k items does it consider those contining K + 1 items. For each k, all candidates are tested to determine whether they have enough support.

A frequent itemset is a set of items that occur together frequently enough to be considered as a candidate for generating association rules.

## 3 Measures: Support, Confidence, Lift

__Support__ is a measure of how frequently the  items must appear in the whole dataset before  they can be considered as a candidate asso-  ciation rule.

__Support__ for a collection of items is the proportion of all transactions in which the items appear together

* support(A -> C) = P(A U C)

We use small values of __support__ as are not looking for the obvious ones.  The actual association rules that we retain are those that meet a criterion __confidence__. 

__Confidence__ calculates the proportion of transactions containing A that also contain C.

* confidence(A -> C) = P(C|A) = P(A U C)/P(A)

* confidence(A -> C) = support(A -> C)/support(A)

Typically looking for larger values of confidence

__Lift__ is the increased likelihood of C being in a transaction if A is included in the transaction:

* lift(A -> C) = confidence(A -> C)/support(C)

Another measure: __Leverage__, which captures the fact that a higher frequency of A and C with a lower lift may be interesting:

* leverage(A->C) = support(A->C)-support(A) \* support(C)

## Titanic dataset

```{r}
data("Titanic")
df <- as.data.frame(Titanic)

# make new object to hold data
titanic.raw <- NULL

# restructure it as a character matrix
for(i in 1:4) {
   titanic.raw <- cbind(titanic.raw, rep(as.character(df[,i]), df$Freq))
}

titanic.raw <- as.data.frame(titanic.raw)
names(titanic.raw) <- names(df)[1:4]

rules.all <- apriori(titanic.raw)

rules.all

inspect(rules.all)
```

As a common phenomenon for association rule mining, many rules generated above are uninteresting. Suppose that we are interested in only rules with rhs indicating survival, so we set rhs=c("Survived=No", "Survived=Yes") in appearance to make sure that only "Survived=No" and "Survived=Yes" will appear in the rhs of rules. All other items can appear in the lhs, as set with default="lhs". 

In the above result rules.all, we can also see that the left-hand side (lhs) of the first rule is empty. To exclude such rules, we set minlen to 2 in the code below. Moreover, the details of the progress are suppressed with verbose=F. 

After association rule mining, rules are sorted by lift to make high-lift rules appear first.

```{r}
rules <- apriori(titanic.raw, control = list(verbose=F),
                 parameter = list(minlen=2, supp=0.005, conf=0.8),
                 appearance = list(rhs=c("Survived=No", 
                                         "Survived=Yes"),
                                   default="lhs"))

quality(rules) <- round(quality(rules), digits=3)
rules.sorted <- sort(rules, by="lift")

inspect(rules.sorted)
```

When other settings are unchanged, with a lower minimum support, more rules will be produced, and the associations between itemsets shown in the rules will be more likely to be by chance. In the above code, the minimum support is set to 0.005, so each rule is supported at least by 12 (=ceiling(0.005 * 2201)) cases, which is acceptable for a population of 2201. 

__Support__, __confidence__ and __lift__ are three common measures for selecting interesting association rules. Besides them, there are many other interestingness measures, such as 
* chi-square
* conviction
* gini
* leverage

More than 20 measures can be calculated with function `interestMeasure()` in the arules package.

```{r}
interestMeasure(rules.sorted,  transactions = titanic.raw)

quality(rules) <- 
  interestMeasure(rules.sorted,  transactions = titanic.raw)

data("Income")
rules <- apriori(Income)

## calculate a single measure and add it to the quality slot
quality(rules) <- cbind(quality(rules), 
                        hyperConfidence = interestMeasure(rules, measure = "hyperConfidence", 
                                                          transactions = Income))

inspect(head(rules, by = "hyperConfidence"))
```

Some rules generated in the previous section (see rules.sorted, page 89) provide little or no extra information when some other rules are in the result. 

For example, the above rule 2 provides no extra knowledge in addition to rule 1, since rules 1 tells us that all 2nd-class children survived.  Generally speaking, when a rule (such as rule 2) is a super rule of another rule (such as rule 1) and the former has the same or a lower lift, the former rule (rule 2) is considered to be redundant.  Other redundant rules in the above result are rules 4, 7 and 8, compared respectively with rules 3, 6 and 5. 

Below we prune redundant rules. Note that the rules have already been sorted descendingly by lift.

To sum up redundancy, if we see that the confidence of Class = 1st, Sex = Female is 1, then by adding if its an adult or an infant does not matter as it does not add information

```{r}
### Just make 
prune_arule <- function(rules, verbose = T) {
  
  rules <- sort(rules, by="lift")
  subset.matrix <- is.subset(rules.sorted, rules.sorted)
  subset.matrix[lower.tri(subset.matrix, diag = T)] <- NA
  redundant <- colSums(subset.matrix, na.rm = T) >= 1
  if(verbose)
  cat("Removing rules:\n", 
      which(redundant), "\n",
      paste(names(which(redundant)),"\n"))
  
  rules[!redundant]
}

rules.pruned <- prune_arule(rules)

inspect(rules.pruned)
```

__While it is easy to find high-lift rules from data, it is not an easy job to understand the identified rules__. It is not uncommon that the association rules are misinterpreted to find their business meanings.  

For instance, in the above rule list `rules.pruned`, the first rule `{Class=2nd, Age=Child} => {Survived=Yes}` has a confidence of one and a lift of three and there are no rules on children of the 1st or 3rd classes. Therefore, it might be interpreted by users as children of the 2nd class had a higher survival rate than other children. __This is wrong!__ 

* The rule states only that all children of class 2 survived, but provides no information at all to compare the survival rates of different classes. 

To investigate the above issue, we run the code below to find rules whose rhs is `Survived=Yes` and lhs contains `Class=1st`, `Class=2nd`, `Class=3rd`, `Age=Child` and `Age=Adult` only, and which contains no other items (default=`none`). We use lower thresholds for both support and confidence than before to find all rules for children of different classes.

* Remember to also change `supp` and `conf` as we work with different samples in the data
* This will give us a much clearer view of the proportional survival rates between classes

```{r}
# create a function to run a truncated apriori - work out supp and conf - input list of rhs and lhs variables
# function(trunc_list, supp, conf)

rules <- apriori(titanic.raw, 
                 parameter = list(minlen=3, supp=0.002, conf=0.2),
                 appearance = list(rhs=c("Survived=Yes"),
                                   lhs=c("Class=1st", "Class=2nd", "Class=3rd",
                                         "Age=Child", "Age=Adult"),
                                   default="none"), 
                 control = list(verbose=F))

rules.sorted <- rules %>% sort(by="confidence")

inspect(rules.sorted)
```

# Visualizing arules

Here we show some ways to visualize association rules, including scatter plot, balloon plot, graph and parallel coordinates plot. More examples on visualizing association rules can be found in the vignettes of package arulesViz [Hahsler and Chelluboina, 2012] which is included in your folder of class materials for today.

```{r}
library(arulesViz)

# figure 9.1 in PDF
plot(rules.all) 

# figure 9.2 in PDF
plot(rules.all, method="grouped")

# figure 9.3 in PDF
plot(rules.all, method="graph")

# figure 9.4
plot(rules.all, method="graph", control=list(type="items"))

# figure 9.5
plot(rules.all, method="paracoord", control=list(reorder=TRUE))


plot(rules.all, 
     method = "matrix", 
     measure = c("lift", "confidence"), 
     control = list(reorder=TRUE))

plot(rules, 
     method = "matrix3D", 
     measure = "lift", 
     control = list(reorder=TRUE))
```

## Groceries Example

```{r}
# Groceries contains sales data of groceries
# with 9835 transactions and 169 items.
data("Groceries")
summary(Groceries)

# Whole milk most popular. Average transaction
# has less than 5 items.

# We mine association rules using
# Apriori algorithm implemented in arules

rules <- apriori(Groceries,
                 parameter=list(support=0.001,
                                confidence=0.5))

rules %>% 
  sort(by = "lift") %>% 
  head(3) %>% 
  inspect
```

Here we see that when people buy `Instant food products, soda`, it is ~19 more likely (`lift`) on average that they will buy `hamburger meat`

Generic Plot Syntax is:
```{r}
plot(x, method = NULL, measure = "support",
            shading = "lift", interactive = FALSE,
            data = data_set, control = list(reorder=TRUE))
```

* x = set of rules to be visualized
* method = visualization method
* measure and shading = interest measures for plot
* interactive = if we want to interactively explore or just present the rules
* data = transaction data set used to mine rules control = list with further control arguments.

```{r}
plot(rules)

rules %>% quality %>% head

rules %>% 
  plot(., 
     measure=c("support", "lift"), 
     shading="confidence")
```

In a "two-key plot", support and confidence are used for the x and y-axes and the color of the points is used to indicate "order" which is the number of items contained in the rule.

```{r}
rules %>% 
  plot(., 
       shading="order", 
       control=list(main="Two-key plot"))
```

## Last FM example

We look for association rules such as: A = LHS  (buy chips) → B = RHS (buy beer). 

* The left-hand side is the `antecedent`
* the right-hand side is the `consequent`

P(B|A) is the conditional probability of B given A. It expresses the probability of event B (the RHS: buying beer), knowing that event A (the LHS: buying chips) has occurred. Recall from your study of probability that P(B|A) = P(A and B)/P(A).

___SUPPORT:___

P(A): Probability that product A is being purchased (event A)—The proportion of times event A occurred is also referred to as the support of A. It is the relative frequency of 1s in column A of the incidence matrix.

P(B): Probability that product B is being purchased (event B)—The proportion of times event B occurred is referred to as the support of B. It is the relative frequency of 1s in column B of the incidence matrix

P(A and B): Probability that products A and B are being purchased at the same time—The proportion of times events A and B occurred together is referred to as the support of A and B. It is the relative frequency of having 1s (i.e., of co-incidence) in both columns A and B of the incidence matrix.

___CONFIDENCE:___

The conditional probability of B (RHS) given A (LHS) is referred to as the confidence of B. It expresses our confidence that product B gets bought if A has been purchased. If this is a small number, the relationship between antecedent A and consequent B is not very relevant as in this case B is unlikely to occur. The confidence is calculated as the ratio, supp(A = LHS and B = RHS)/supp(A = LHS), with the supports of these two events being obtained from the incidence matrix.

___LIFT:___

The lift of A on B is defined as the ratio: 

* lift(A → B) = P(B|A) / P(B) = P(A and B) / P(A)P(B). 

It compares P(B|A) with P(B). If this ratio is larger than 1, we say that A (LHS) results in an upward lift on B (RHS). Or, knowing that A (the antecedent) has occurred, increases the chance that B (the consequent) occurs. 

The lift of A (LHS) on B (RHS) is calculated as the ratio supp(LHS and RHS)/[supp(LHS)supp(RHS)]. Note that this is the same as the lift of B on A:

* P(A|B)/P(A) = P(A and B)/P(A)P(B).

### Balance between lift and confidence

You want to find antecedents that result in big lifts. But, big lifts are practically relevant only if the consequent has a reasonable chance of occurring. Hence, it is productive to screen for combinations that result in good lift AND high confidence.

```{r}
lastfm <- read.csv("Materials-Session-3-More-Association-Analysis/data/lastfm.csv") %>% 
  distinct(., .keep_all = T)
```

Lets use this to now create a playlist for each of the people and get unique records. We must remember that apriori can only handle `factors`
```{r}

playlist <- 
  lastfm %>% 
  mutate(user = factor(user)) %>% 
  split_by("user") %>% 
  map(~ distinct(.x))

playlist <- split(x=lastfm[,"artist"], f=lastfm$user)
playlist <- lapply(playlist,unique) 
```

Lets look at the first two users and the music they listen to:
```{r}
playlist[c(1,2)]
```

The cool thing about apriori and `arules` is the fact that it can create a transaction matrix from my list of users and they wine they enjoy
```{r}
playlist <- as(playlist,"transactions")
```

```{r}
itemFrequency(playlist)

itemFrequencyPlot(playlist, support = 0.08, cex.names = 1.5)

```

Lets only get build the priori rule so that only rules with support >0.01 and confidence of > 0.50 exists, so we don't have a super rare band that had a one hit wonder

```{r}
musicrules <- apriori(playlist,
                      parameter=list(support=.01,confidence=.5)) 

inspect(musicrules)
```


Filter then only those who had a lift of > 5

```{r}
inspect(subset(musicrules, subset=lift > 5)) 

inspect(sort(subset(musicrules, subset=lift > 5), 
             by="confidence"))
```

Here we see that if we were to recommend pink floyd to those who listen to led zeppelin and the doors, then we would be correct 60% of the time

## Income Example

As second example we use the Adult data set from the UCI machine learning repository; 

This data set was analyzed by Hahsler et al. (2005) in their illustration of the R package arules. The data, taken from the US Census Bureau database, contains 48,842 individuals with data on income and several possible predictors of income such as age, work class, education, and so on. 

The first part of the R program in their paper (copied below) transforms the original variables into 115 binary indicator variables. Income, for example is coded as "small" (US$50,000 or less) and "large" (US$50,000 or more). We skip these details and focus our attention on the resulting transaction (incidence) matrix Adult with its 48,842 rows and 115 columns. Since the dimensions of the matrix are so large, the matrix is stored in a special, sparse, transaction matrix format. The item (column) with the largest number of 1s is "capital − loss = none" (46,560 out of 48,842). The number of 1s on a subject (also referred to as the length) varies between 9 and 13; this narrow band is not surprising as indicator variables were constructed from categorical variables, and we know that summing over indicators of each categorical variable must give one for every subject. But, it is the way how these 0s and 1s coincide that matters. We get a better understanding of how the strings of zeros and ones look like by considering the incidence matrix (which we create from the transaction matrix). The indicator for small income (0/1) is in the penultimat column; the indicator for large income is in the very last column.

__NOTE__ the awesome ordered variable

```{r}

data(AdultUCI)

# look at first three rows
AdultUCI[1:3,]

# remove columns we do not need
AdultUCI <- AdultUCI %>% 
  select(-c(fnlwgt, `education-num`))

# AdultUCI[["fnlwgt"]] <- NULL
# AdultUCI[["education-num"]] <- NULL


AdultUCI[["age"]] <- 
  ordered(cut(AdultUCI[["age"]], 
              c(15, 25, 45, 65, 100)), 
          labels = c("Young", "Middle-aged", "Senior", "Old"))

AdultUCI[["hours-per-week"]] <- 
  ordered(cut(AdultUCI[["hours-per-week"]], 
              c(0, 25, 40, 60, 168)), 
          labels = c("Part-time", "Full-time", "Over-time", "Workaholic"))

AdultUCI[["capital-gain"]] <- 
  ordered(cut(AdultUCI[["capital-gain"]], 
              c(-Inf, 0, median(AdultUCI[["capital-gain"]][AdultUCI[["capital-gain"]] > 0]), Inf)), 
          labels = c("None", "Low", "High"))

AdultUCI[["capital-loss"]] <- 
  ordered(cut(AdultUCI[["capital-loss"]], 
              c(-Inf, 0, median(AdultUCI[["capital-loss"]][AdultUCI[["capital-loss"]] > 0]), Inf)), 
          labels = c("none", "low", "high"))
```

Lets transform this data into a sparse matrix and then into incidence matrix

```{r}
# make it a sparse format
Adult <- as(AdultUCI, "transactions")
Adult
summary(Adult)

# transforms transaction matrix into incidence matrix
aa <- as(Adult,"matrix") 

# print the first two rows of the incidence matrix
aa[1:2,]
# look at frequencies of items
itemFrequencyPlot(Adult[, itemFrequency(Adult) > 0.2], cex.names = 1)
```

Now that we have the sparse matrix, lets run the apriori algorithm

```{r}
rules <- apriori(Adult, 
                 parameter = list(support = 0.01, 
                                  confidence = 0.6))
# how many of them?
rules
# summary of rules
summary(rules)
```

Finally, we search over a subset of all rules (with support greater than 0.01 and confidence greater than 0.60, as specified previously) that have small income on the RHS and achieve a lift larger than 1.2. 

```{r}
rulesIncomeSmall <- subset(rules, subset = rhs %in% "income=small" & lift > 1.2)
inspect(sort(rulesIncomeSmall, by = "confidence")[1:3])

rulesIncomeLarge <- subset(rules, subset = rhs %in% "income=large" & lift > 1.2)
inspect(sort(rulesIncomeLarge, by = "confidence")[1:3])
```


### How to sample

To calculate a reasonable sample size n, we choose a minimum support of 5% tied into the formula you see below. As an acceptable error rate for support we choose 10% and as the confidence level (1 − c) we choose 90%.

```{r}
supp <- 0.05
epsilon <- 0.1
c <- 0.1
n <- -2 * log(c)/ (supp * epsilon^2)
n
```

# Social network analysis
```{r}
g <- graph(c(1,2, 1,3, 2,3, 3,5), n=5)
# and then plot it
plot(g)
```

We are assigning to the variable g a graph that has nodes 

* V = {1,2,3,4,5} (from n=5) and has edges 
* E = {(1,2), (1,3), (2,3), (3,5)} 

The commands `V(g)` and `E(g)` print the list of nodes and edges of the graph g:

```{r}

# vertex sequence:
V(g)

# edge sequence
E(g)

```

```{r}
# You can add nodes and edges to an already
# existing graph by, for example, doing this:
g <- graph.empty() + vertices(letters[1:10], 
                              color="red")
# it creates an empty graph and then adds vertices.
# take a look:
plot(g)
# we add more:
g <- g + vertices(letters[11:20], 
                  color="blue")
# take a look:
plot(g)
# We add edges sampling from the existing
# edges in g:
g <- g + edges(sample(V(g), 30, replace=TRUE), 
               color="green")
# Take a look:
plot(g)

# resulting Vertex sequence:
V(g)

# resulting Edge sequence
E(g)
```


## Loading graphs & GRAPH GENERATORS

Lets load in a "pajek" graph type - they are made for very large graphic networks

```{r}
karate <- read.graph("http://cneurocvs.rmki.kfki.hu/igraph/karate.net", format = "pajek")
plot(karate)
```

igraph implements also many useful graph generators. There are models such as: 

* Edos-Renyi model (ER)
* Barabasi-Albert model (BA)
* Watts-Strogratz model (WS). 

The following commands generate graphs using these models:

```{r}
er_graph <- erdos.renyi.game(100, 2/100)
plot(er_graph)
ws_graph <- watts.strogatz.game(1, 100, 4, 0.05)
plot(ws_graph)
ba_graph <- barabasi.game(100)
plot(ba_graph)

```

We can add attributes to nodes and edges of the graphs. These are useful for selecting certain types of nodes, and for visualization. What these commands do is to generate a random graph with 10 nodes, assigns random colors to the nodes, colors edges joining red nodes in red, and edges joining black nodes in black. All remaining edges are colored grey.

```{r}
g <- erdos.renyi.game(10, 0.5)
plot(g)
# assigns random colors to nodes
V(g)$color <- sample( c("red", "black"), vcount(g), rep=TRUE)
plot(g)

# assign grey to the edges
E(g)$color <- "grey"
plot(g)

# identify red vertices as variable "red"
red <- V(g)[ color == "red" ]
# no changes:
plot(g)

# identify black vertices as variable "bl"
bl <- V(g)[ color == "black" ]
# no changes:
plot(g)

# make edges for red nodes red:
E(g)[ red %--% red ]$color <- "red"

# make edges for black nodes black:
E(g)[ bl %--% bl ]$color <- "black"

# remaining edges are still grey:
plot(g)
```

What the following set of commands do is this: They assign random weights to a lattice graph and then colors the ones having weight over 0.9 red, and the rest grey.

```{r}
g <- graph.lattice( c(10,10) )
# graph that looks like a lattice:
plot(g)
# assign random weights:
E(g)$weight <- runif(ecount(g))
# color all edges grey:
E(g)$color <- "grey"
# plot g:
plot(g)
# color it red if weight > 0.9:
E(g)[ weight > 0.9 ]$color <- "red"
# show g:
plot(g)

```

A very important part in the analysis of networks is being able to visualize them As an example, consider the following commands which render the three graphs depicted below:

```{r}
er_graph <- erdos.renyi.game(100, 2/100)
plot(er_graph, vertex.label=NA, vertex.size=3)

ws_graph <- watts.strogatz.game(1, 100, 4, 0.05)
plot(ws_graph, layout=layout.circle, 
     vertex.label=NA, vertex.size=3)

ba_graph <- barabasi.game(100)
plot(ba_graph, vertex.label=NA, vertex.size=3)

```

